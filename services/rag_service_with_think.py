from core.factory import AppFactory
from langchain_core.prompts import ChatPromptTemplate
from core.models import TestCases
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnablePassthrough
import json
import re
import streamlit as st

def clean_and_parse_json(text):
    """
    æ¸…ç† LLM è¾“å‡ºå¹¶æå– JSON å†…å®¹
    """
    # ç§»é™¤å¯èƒ½çš„æ€è€ƒæ ‡ç­¾å’Œå…¶ä»–éJSONå†…å®¹
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'<.*?>', '', text)
    
    # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡
    json_match = re.search(r'\{.*\}', text, flags=re.DOTALL)
    if json_match:
        json_str = json_match.group()
        try:
            # å°è¯•ç›´æ¥è§£æ
            parsed = json.loads(json_str)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤æ ¼å¼
            if 'test_cases' in parsed and isinstance(parsed['test_cases'], list):
                # æ£€æŸ¥æ•°ç»„å…ƒç´ æ˜¯å¦ä¸ºå­—ç¬¦ä¸²æ ¼å¼çš„TestCaseå¯¹è±¡
                fixed_cases = []
                for case in parsed['test_cases']:
                    if isinstance(case, str) and case.startswith('TestCase('):
                        # è¿™æ˜¯å­—ç¬¦ä¸²æ ¼å¼çš„TestCaseï¼Œéœ€è¦è§£æ
                        try:
                            # æå–TestCaseä¸­çš„å‚æ•°
                            case_content = case[9:-1]  # ç§»é™¤ 'TestCase(' å’Œ ')'
                            case_dict = {}
                            
                            # ç®€å•çš„å‚æ•°è§£æ
                            params = re.findall(r"(\w+)='([^']*)'", case_content)
                            for key, value in params:
                                case_dict[key] = value
                            
                            fixed_cases.append(case_dict)
                        except:
                            # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªç”¨ä¾‹
                            continue
                    elif isinstance(case, dict):
                        fixed_cases.append(case)
                
                parsed['test_cases'] = fixed_cases
            
            return parsed
        except json.JSONDecodeError:
            pass
    
    # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError as e:
        raise ValueError(f"æ— æ³•è§£æJSONè¾“å‡º: {str(e)}\nåŸå§‹è¾“å‡º: {text[:500]}...")

def extract_thinking_content(text):
    """
    æå–æ€è€ƒè¿‡ç¨‹å†…å®¹
    """
    # æŸ¥æ‰¾ <think> æ ‡ç­¾å†…å®¹
    think_match = re.search(r'<think>(.*?)</think>', text, flags=re.DOTALL)
    if think_match:
        return think_match.group(1).strip()
    return None

def generate_test_cases_with_rag_and_think(model_settings, num_cases, requirement, kb_manager, enable_think=False, think_container=None):
    """
    ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œæ”¯æŒ think åŠŸèƒ½å’Œæ‰¹æ¬¡ç”Ÿæˆ
    
    Args:
        model_settings: æ¨¡å‹é…ç½®
        num_cases: ç”¨ä¾‹æ•°é‡
        requirement: éœ€æ±‚æè¿°
        kb_manager: çŸ¥è¯†åº“ç®¡ç†å™¨
        enable_think: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
        think_container: Streamlit å®¹å™¨ï¼Œç”¨äºæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
    """
    # åŠ¨æ€ç¡®å®šæ‰¹æ¬¡å¤§å°
    BATCH_SIZE = get_optimal_batch_size(model_settings)
    
    # å¦‚æœç”¨ä¾‹æ•°é‡è¶…è¿‡æ‰¹æ¬¡å¤§å°ï¼Œä½¿ç”¨æ‰¹æ¬¡ç”Ÿæˆ
    if num_cases > BATCH_SIZE:
        return generate_test_cases_in_batches(
            model_settings, num_cases, requirement, kb_manager, 
            enable_think, think_container
        )
    
    # å•æ‰¹æ¬¡ç”Ÿæˆ
    return generate_single_batch(
        model_settings, num_cases, requirement, kb_manager, 
        enable_think, think_container, 1, []
    )

def get_optimal_batch_size(model_settings):
    """
    æ ¹æ®æ¨¡å‹æä¾›å•†å’Œæ¨¡å‹åç§°ç¡®å®šæœ€ä¼˜æ‰¹æ¬¡å¤§å°
    """
    provider = model_settings.get('provider', '').lower()
    model_name = model_settings.get('model_name', '').lower()
    
    # Ollama æœ¬åœ°æ¨¡å‹ - è¾ƒå°çš„æ‰¹æ¬¡å¤§å°ï¼ˆå—æœ¬åœ°è®¡ç®—èµ„æºé™åˆ¶ï¼‰
    if provider == 'ollama':
        if 'qwen3:4b' in model_name or 'qwen3:8b' in model_name:
            return 5  # å°æ¨¡å‹ï¼Œä¿æŒè¾ƒå°æ‰¹æ¬¡
        elif any(size in model_name for size in ['4b', '7b', '8b']):
            return 8  # ä¸­ç­‰æ¨¡å‹
        else:
            return 10  # å¤§æ¨¡å‹
    
    # è¿œç¨‹APIè°ƒç”¨ - æ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡èƒ½åŠ›é€‚åº¦æ”¾å¼€é™åˆ¶ï¼ˆè€ƒè™‘å®é™…æ€§èƒ½ï¼‰
    elif provider in ['openaicompatible', 'openai', 'gemini', 'qwen', 'doubao']:
        # Qwenç³»åˆ—ï¼šæ”¯æŒ1Mä¸Šä¸‹æ–‡ï¼Œä½†è€ƒè™‘å®é™…ç”Ÿæˆé€Ÿåº¦
        if 'qwen' in model_name:
            return 20  # Qwenç³»åˆ—é€‚åº¦å¢åŠ æ‰¹æ¬¡ï¼Œé¿å…è¶…æ—¶
        
        # Doubaoç³»åˆ—ï¼šæ”¯æŒ256Kä¸Šä¸‹æ–‡ï¼Œé€‚åº¦å¢åŠ 
        elif 'doubao' in model_name or provider == 'doubao':
            return 15  # Doubaoç³»åˆ—é€‚åº¦å¢åŠ æ‰¹æ¬¡
        
        # GPT-4ç³»åˆ—ï¼šæ”¯æŒ128Kä¸Šä¸‹æ–‡
        elif 'gpt-4' in model_name:
            return 15  # GPT-4é€‚åº¦å¢åŠ æ‰¹æ¬¡
        
        # Claudeç³»åˆ—ï¼šæ”¯æŒ200Kä¸Šä¸‹æ–‡
        elif 'claude' in model_name:
            return 15  # Claudeé€‚åº¦å¢åŠ æ‰¹æ¬¡
        
        # GPT-3.5å’Œå…¶ä»–æ¨¡å‹ï¼šæ ‡å‡†ä¸Šä¸‹æ–‡
        elif 'gpt-3.5' in model_name:
            return 12  # GPT-3.5æ ‡å‡†æ‰¹æ¬¡
        
        else:
            return 10  # å…¶ä»–è¿œç¨‹æ¨¡å‹é»˜è®¤æ‰¹æ¬¡
    
    # é»˜è®¤å€¼ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
    return 8

def generate_test_cases_in_batches(model_settings, num_cases, requirement, kb_manager, enable_think, think_container):
    """
    åˆ†æ‰¹æ¬¡ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œå¸¦å»é‡æœºåˆ¶
    """
    batch_size = get_optimal_batch_size(model_settings)  # åŠ¨æ€ç¡®å®šæ‰¹æ¬¡å¤§å°
    all_test_cases = []
    batches = []
    
    # è®¡ç®—æ‰¹æ¬¡
    remaining_cases = num_cases
    batch_num = 1
    
    while remaining_cases > 0:
        current_batch_size = min(batch_size, remaining_cases)
        batches.append((batch_num, current_batch_size))
        remaining_cases -= current_batch_size
        batch_num += 1
    
    # å¦‚æœæœ‰æ€è€ƒå®¹å™¨ï¼Œæ˜¾ç¤ºæ‰¹æ¬¡ä¿¡æ¯
    if think_container:
        with think_container:
            provider_name = model_settings.get('provider', 'Unknown')
            model_name = model_settings.get('model_name', 'Unknown')
            st.info(f"ğŸ¤– ä½¿ç”¨ {provider_name} - {model_name}")
            st.info(f"ğŸ“¦ æ£€æµ‹åˆ°éœ€è¦ç”Ÿæˆ {num_cases} æ¡ç”¨ä¾‹ï¼Œå°†åˆ† {len(batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œï¼Œæ¯æ‰¹æ¬¡æœ€å¤š {batch_size} æ¡")
            st.info("ğŸ”„ ç³»ç»Ÿå°†è‡ªåŠ¨é¿å…ç”Ÿæˆé‡å¤çš„æµ‹è¯•ç”¨ä¾‹")
            
            # åˆ›å»ºæ‰¹æ¬¡è¿›åº¦æ¡
            batch_progress = st.progress(0)
            batch_status = st.empty()
    
    # é€æ‰¹æ¬¡ç”Ÿæˆ
    for i, (batch_num, current_batch_size) in enumerate(batches):
        if think_container:
            batch_status.info(f"ğŸ”„ æ­£åœ¨ç”Ÿæˆç¬¬ {batch_num} æ‰¹æ¬¡ ({current_batch_size} æ¡ç”¨ä¾‹)...")
            batch_progress.progress((i) / len(batches))
        
        try:
            # ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºç‹¬ç«‹çš„æ€è€ƒå®¹å™¨
            batch_think_container = None
            if think_container and enable_think:
                with think_container:
                    with st.expander(f"ğŸ¤” ç¬¬ {batch_num} æ‰¹æ¬¡æ€è€ƒè¿‡ç¨‹", expanded=False):
                        batch_think_container = st.container()
            
            # ç”Ÿæˆå½“å‰æ‰¹æ¬¡ï¼ˆä¼ é€’å·²ç”Ÿæˆçš„ç”¨ä¾‹ä¿¡æ¯ï¼‰
            batch_result = generate_single_batch(
                model_settings, current_batch_size, requirement, kb_manager,
                enable_think, batch_think_container, batch_num, all_test_cases
            )
            
            # åˆå¹¶ç»“æœå¹¶å»é‡
            if batch_result and hasattr(batch_result, 'test_cases'):
                # å»é‡å¤„ç†
                new_cases = deduplicate_test_cases(batch_result.test_cases, all_test_cases)
                all_test_cases.extend(new_cases)
                
                if think_container:
                    original_count = len(batch_result.test_cases)
                    final_count = len(new_cases)
                    if original_count > final_count:
                        batch_status.warning(f"âš ï¸ ç¬¬ {batch_num} æ‰¹æ¬¡å»é‡ï¼šåŸç”Ÿæˆ {original_count} æ¡ï¼Œå»é‡å {final_count} æ¡")
                    else:
                        batch_status.success(f"âœ… ç¬¬ {batch_num} æ‰¹æ¬¡å®Œæˆï¼Œç”Ÿæˆäº† {final_count} æ¡ç”¨ä¾‹")
            else:
                if think_container:
                    batch_status.error(f"âŒ ç¬¬ {batch_num} æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥")
                
        except Exception as e:
            if think_container:
                batch_status.error(f"âŒ ç¬¬ {batch_num} æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥: {str(e)}")
            continue
    
    # å®Œæˆæ‰€æœ‰æ‰¹æ¬¡
    if think_container:
        batch_progress.progress(1.0)
        batch_status.success(f"ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å®Œæˆï¼æ€»å…±ç”Ÿæˆäº† {len(all_test_cases)} æ¡æµ‹è¯•ç”¨ä¾‹")
    
    # è¿”å›åˆå¹¶åçš„ç»“æœï¼Œå¹¶æ¸…ç†nanå€¼
    from core.models import TestCases, TestCase
    # ç¡®ä¿all_test_casesä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯TestCaseå®ä¾‹ï¼Œå¹¶æ¸…ç†nanå€¼
    processed_cases = []
    for case in all_test_cases:
        # è·å–å­—å…¸å½¢å¼çš„æ•°æ®
        if hasattr(case, 'model_dump'):
            case_dict = case.model_dump()
        elif hasattr(case, 'dict'):
            case_dict = case.dict()
        else:
            case_dict = case
        
        # æ¸…ç†å­—å…¸ä¸­çš„nanå€¼
        cleaned_case_dict = {}
        for key, value in case_dict.items():
            if value is None or str(value).lower() in ['nan', 'none', 'null']:
                cleaned_case_dict[key] = ""
            else:
                cleaned_case_dict[key] = str(value) if value is not None else ""
        
        # åˆ›å»ºæ¸…ç†åçš„TestCaseå®ä¾‹
        processed_cases.append(TestCase(**cleaned_case_dict))
    
    return TestCases(test_cases=processed_cases)

def deduplicate_test_cases(new_cases, existing_cases):
    """
    å»é‡å‡½æ•°ï¼šç§»é™¤ä¸å·²æœ‰ç”¨ä¾‹é‡å¤çš„æµ‹è¯•ç”¨ä¾‹
    """
    if not existing_cases:
        return new_cases
    
    unique_cases = []
    existing_titles = {case.ç”¨ä¾‹åç§°.strip().lower() for case in existing_cases}
    existing_descriptions = {case.æ­¥éª¤æè¿°.strip().lower() for case in existing_cases}
    
    for case in new_cases:
        title_lower = case.ç”¨ä¾‹åç§°.strip().lower()
        desc_lower = case.æ­¥éª¤æè¿°.strip().lower()
        
        # æ£€æŸ¥æ ‡é¢˜å’Œæè¿°æ˜¯å¦é‡å¤
        is_duplicate = (
            title_lower in existing_titles or
            desc_lower in existing_descriptions or
            any(similarity_check(title_lower, existing_title) > 0.8 for existing_title in existing_titles) or
            any(similarity_check(desc_lower, existing_desc) > 0.8 for existing_desc in existing_descriptions)
        )
        
        if not is_duplicate:
            unique_cases.append(case)
            existing_titles.add(title_lower)
            existing_descriptions.add(desc_lower)
    
    return unique_cases

def similarity_check(text1, text2):
    """
    ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æŸ¥
    """
    if not text1 or not text2:
        return 0
    
    # ç®€å•çš„å­—ç¬¦çº§ç›¸ä¼¼åº¦
    set1 = set(text1)
    set2 = set(text2)
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    
    return intersection / union if union > 0 else 0

def generate_single_batch(model_settings, num_cases, requirement, kb_manager, enable_think, think_container, batch_num=1, existing_cases=None):
    """
    ç”Ÿæˆå•ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•ç”¨ä¾‹
    """
    llm_integrator = AppFactory.create_llm_integrator(
        model_settings['provider'],
        model_settings.get('model_name', 'qwen3:4b'),
        model_settings['api_key'],
        model_settings['base_url']
    )
    
    # è·å– LLMï¼Œå¦‚æœæ˜¯ Qwen3 æ¨¡å‹ä¸”å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œåˆ™æ·»åŠ ç›¸åº”é…ç½®
    llm = llm_integrator.get_llm()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ Qwen3 æ¨¡å‹
    is_qwen3 = 'qwen3' in model_settings.get('model_name', '').lower()
    
    retriever = kb_manager.get_retriever()
    parser = JsonOutputParser(pydantic_object=TestCases)

    # æ„å»ºæç¤ºè¯ï¼Œæ ¹æ®æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼è°ƒæ•´
    think_instruction = ""
    if enable_think and is_qwen3:
        think_instruction = "/think\n"
    elif not enable_think and is_qwen3:
        think_instruction = "/no_think\n"

    # æ„å»ºå·²ç”Ÿæˆç”¨ä¾‹çš„æ‘˜è¦ä¿¡æ¯
    existing_info = ""
    if existing_cases and len(existing_cases) > 0:
        existing_titles = [case.ç”¨ä¾‹åç§° for case in existing_cases[-10:]]  # åªæ˜¾ç¤ºæœ€è¿‘10ä¸ª
        existing_info = f"""
ã€å·²ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ‘˜è¦ã€‘ï¼ˆç¬¬{batch_num}æ‰¹æ¬¡ï¼Œè¯·é¿å…é‡å¤ï¼‰
å‰é¢æ‰¹æ¬¡å·²ç”Ÿæˆçš„ç”¨ä¾‹æ ‡é¢˜ï¼š
{chr(10).join(f"- {title}" for title in existing_titles)}

ã€é‡è¦ã€‘è¯·ç¡®ä¿æœ¬æ‰¹æ¬¡ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ä¸ä¸Šè¿°å·²æœ‰ç”¨ä¾‹ä¸é‡å¤ï¼Œè¦æœ‰ä¸åŒçš„æµ‹è¯•è§’åº¦å’Œåœºæ™¯ã€‚
"""

    template = f"""{think_instruction}ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æµ‹è¯•å·¥ç¨‹å¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚æè¿°å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆ {{num_cases}} ä¸ªç»“æ„åŒ–çš„é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ã€‚

ã€é‡è¦æŒ‡ä»¤ã€‘
1. ä¼˜å…ˆä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆæ›´è´´åˆ‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚
2. è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªåä¸º 'test_cases' çš„é”®ï¼Œå…¶å€¼æ˜¯æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ã€‚
3. æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹éƒ½å¿…é¡»åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µã€‚
4. è¯·ä½¿ç”¨ä¸­æ–‡ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å†…å®¹ã€‚
5. æœ€ç»ˆè¾“å‡ºåªåŒ…å«JSONæ ¼å¼çš„æµ‹è¯•ç”¨ä¾‹ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ–‡æœ¬ã€‚
6. ç¡®ä¿æµ‹è¯•ç”¨ä¾‹å…·æœ‰å¤šæ ·æ€§ï¼Œè¦†ç›–ä¸åŒçš„æµ‹è¯•åœºæ™¯å’Œè¾¹ç•Œæ¡ä»¶ã€‚
7. æ‰€æœ‰å­—æ®µéƒ½å¿…é¡»æœ‰å€¼ï¼Œä¸èƒ½ä¸ºnullã€undefinedæˆ–ç©ºå€¼ï¼Œå¦‚æœæŸä¸ªå­—æ®µæš‚æ—¶æ²¡æœ‰å†…å®¹ï¼Œè¯·ä½¿ç”¨ç©ºå­—ç¬¦ä¸²""
8. ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œæ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µéƒ½ç”¨åŒå¼•å·åŒ…å›´

ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘
{{context}}

ã€éœ€æ±‚æè¿°ã€‘
{{requirement}}

{existing_info}

{{format_instructions}}

è¯·ç›´æ¥è¾“å‡ºæœ‰æ•ˆçš„JSONæ ¼å¼ï¼š
"""
    
    prompt = ChatPromptTemplate.from_template(
        template,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )

    # åˆ›å»ºé“¾
    rag_chain = (
        {"context": retriever, "requirement": RunnablePassthrough(), "num_cases": RunnablePassthrough()}
        | prompt
        | llm
    )

    # å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ä¸”æä¾›äº†å®¹å™¨ï¼Œä½¿ç”¨æµå¼è¾“å‡º
    if enable_think and is_qwen3 and think_container:
        return generate_with_streaming_think(rag_chain, requirement, num_cases, think_container)
    else:
        # æ ‡å‡†ç”Ÿæˆæ¨¡å¼
        return generate_standard(rag_chain, requirement, num_cases)

def generate_with_streaming_think(rag_chain, requirement, num_cases, think_container):
    """
    å¸¦æœ‰æµå¼æ€è€ƒè¿‡ç¨‹çš„ç”Ÿæˆ
    """
    try:
        # è·å–æµå¼è¾“å‡º
        full_response = ""
        thinking_content = ""
        json_content = ""
        in_thinking = False
        
        with think_container:
            # åˆ›å»ºä¸€ä¸ªå¯å±•å¼€çš„æ€è€ƒè¿‡ç¨‹åŒºåŸŸ
            with st.expander("ğŸ¤” AI æ€è€ƒè¿‡ç¨‹", expanded=True):
                st.info("ğŸ’¡ AI æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚ï¼Œæ€è€ƒè¿‡ç¨‹å°†å®æ—¶æ˜¾ç¤º...")
                
                thinking_placeholder = st.empty()
                progress_bar = st.progress(0)
                
            # è°ƒç”¨é“¾å¹¶å¤„ç†æµå¼è¾“å‡º
            chunk_count = 0
            for chunk in rag_chain.stream({"requirement": requirement, "num_cases": num_cases}):
                if hasattr(chunk, 'content'):
                    content = chunk.content
                    full_response += content
                    chunk_count += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡ï¼ˆä¼°ç®—ï¼‰
                    progress = min(chunk_count * 0.02, 0.9)  # æœ€å¤šåˆ°90%
                    progress_bar.progress(progress)
                    
                    # æ£€æŸ¥æ˜¯å¦è¿›å…¥æ€è€ƒæ¨¡å¼
                    if '<think>' in content:
                        in_thinking = True
                    
                    if in_thinking:
                        thinking_content += content
                        # å®æ—¶æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ - ä½¿ç”¨ç®€æ´çš„æ ¼å¼
                        clean_thinking = re.sub(r'</?think>', '', thinking_content).strip()
                        if clean_thinking:
                            thinking_placeholder.markdown(
                                f"""
                                <div style="
                                    background-color: #f8f9fa;
                                    border: 1px solid #e9ecef;
                                    border-left: 4px solid #28a745;
                                    padding: 20px;
                                    border-radius: 8px;
                                    margin: 15px 0;
                                    font-family: 'SF Mono', 'Monaco', 'Inconsolata', 'Roboto Mono', monospace;
                                    white-space: pre-wrap;
                                    word-wrap: break-word;
                                    max-height: 500px;
                                    overflow-y: auto;
                                    line-height: 1.6;
                                    font-size: 14px;
                                    color: #2c3e50;
                                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                                ">
                                <div style="
                                    color: #28a745;
                                    font-weight: bold;
                                    margin-bottom: 10px;
                                    border-bottom: 1px solid #e9ecef;
                                    padding-bottom: 8px;
                                ">
                                ğŸ§  æ€è€ƒä¸­...
                                </div>
                                <div>
                                {clean_thinking}
                                </div>
                                </div>
                                """,
                                unsafe_allow_html=True
                            )
                    
                    # æ£€æŸ¥æ˜¯å¦é€€å‡ºæ€è€ƒæ¨¡å¼
                    if '</think>' in content:
                        in_thinking = False
                        progress_bar.progress(1.0)  # å®Œæˆ
                        json_content = full_response.split('</think>')[-1] if '</think>' in full_response else ""
        
        # è§£ææœ€ç»ˆç»“æœ
        if json_content.strip():
            parsed_data = clean_and_parse_json(json_content)
        else:
            parsed_data = clean_and_parse_json(full_response)
            
        return TestCases(**parsed_data)
        
    except Exception as e:
        st.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        # å›é€€åˆ°æ ‡å‡†æ¨¡å¼
        return generate_standard(rag_chain, requirement, num_cases)

def generate_standard(rag_chain, requirement, num_cases):
    """
    æ ‡å‡†ç”Ÿæˆæ¨¡å¼
    """
    # è·å–åŸå§‹è¾“å‡ºå¹¶æ‰‹åŠ¨è§£æ
    raw_output = rag_chain.invoke({"requirement": requirement, "num_cases": num_cases})
    
    # æå–æ–‡æœ¬å†…å®¹
    if hasattr(raw_output, 'content'):
        text_output = raw_output.content
    else:
        text_output = str(raw_output)
    
    # ä½¿ç”¨è‡ªå®šä¹‰è§£æå™¨
    try:
        parsed_data = clean_and_parse_json(text_output)
        return TestCases(**parsed_data)
    except Exception as e:
        print(f"è§£æé”™è¯¯: {str(e)}")
        print(f"åŸå§‹è¾“å‡º: {text_output[:1000]}...")
        raise

# ä¿æŒå‘åå…¼å®¹æ€§
def generate_test_cases_with_rag(model_settings, num_cases, requirement, kb_manager):
    """
    åŸæœ‰çš„ç”Ÿæˆå‡½æ•°ï¼Œä¿æŒå‘åå…¼å®¹
    """
    return generate_test_cases_with_rag_and_think(
        model_settings, num_cases, requirement, kb_manager, enable_think=False
    )
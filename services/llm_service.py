from core.factory import AppFactory
from langchain_core.prompts import ChatPromptTemplate
from core.models import TestCases
from langchain_core.output_parsers import JsonOutputParser
import streamlit as st

def get_optimal_batch_size(model_settings):
    """
    æ ¹æ®æ¨¡å‹æä¾›å•†å’Œæ¨¡å‹åç§°ç¡®å®šæœ€ä¼˜æ‰¹æ¬¡å¤§å°
    """
    provider = model_settings.get('provider', '').lower()
    model_name = model_settings.get('model_name', '').lower()
    
    # Ollama æœ¬åœ°æ¨¡å‹ - è¾ƒå°çš„æ‰¹æ¬¡å¤§å°ï¼ˆå—æœ¬åœ°è®¡ç®—èµ„æºé™åˆ¶ï¼‰
    if provider == 'ollama':
        if 'qwen3:4b' in model_name or 'qwen3:8b' in model_name:
            return 5  # å°æ¨¡å‹ï¼Œä¿æŒè¾ƒå°æ‰¹æ¬¡
        elif any(size in model_name for size in ['4b', '7b', '8b']):
            return 8  # ä¸­ç­‰æ¨¡å‹
        else:
            return 10  # å¤§æ¨¡å‹
    
    # è¿œç¨‹APIè°ƒç”¨ - æ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡èƒ½åŠ›é€‚åº¦æ”¾å¼€é™åˆ¶ï¼ˆè€ƒè™‘å®é™…æ€§èƒ½ï¼‰
    elif provider in ['openaicompatible', 'openai', 'gemini', 'qwen', 'doubao']:
        # Qwenç³»åˆ—ï¼šæ”¯æŒ1Mä¸Šä¸‹æ–‡ï¼Œä½†è€ƒè™‘å®é™…ç”Ÿæˆé€Ÿåº¦
        if 'qwen' in model_name:
            return 20  # Qwenç³»åˆ—é€‚åº¦å¢åŠ æ‰¹æ¬¡ï¼Œé¿å…è¶…æ—¶
        
        # Doubaoç³»åˆ—ï¼šæ”¯æŒ256Kä¸Šä¸‹æ–‡ï¼Œé€‚åº¦å¢åŠ 
        elif 'doubao' in model_name or provider == 'doubao':
            return 15  # Doubaoç³»åˆ—é€‚åº¦å¢åŠ æ‰¹æ¬¡
        
        # GPT-4ç³»åˆ—ï¼šæ”¯æŒ128Kä¸Šä¸‹æ–‡
        elif 'gpt-4' in model_name:
            return 15  # GPT-4é€‚åº¦å¢åŠ æ‰¹æ¬¡
        
        # Claudeç³»åˆ—ï¼šæ”¯æŒ200Kä¸Šä¸‹æ–‡
        elif 'claude' in model_name:
            return 15  # Claudeé€‚åº¦å¢åŠ æ‰¹æ¬¡
        
        # GPT-3.5å’Œå…¶ä»–æ¨¡å‹ï¼šæ ‡å‡†ä¸Šä¸‹æ–‡
        elif 'gpt-3.5' in model_name:
            return 12  # GPT-3.5æ ‡å‡†æ‰¹æ¬¡
        
        else:
            return 10  # å…¶ä»–è¿œç¨‹æ¨¡å‹é»˜è®¤æ‰¹æ¬¡
    
    # é»˜è®¤å€¼ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
    return 8

def generate_test_cases_with_llm(model_settings, num_cases, requirement):
    """
    ä½¿ç”¨LLMç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œæ”¯æŒåˆ†æ‰¹æ¬¡ç”Ÿæˆ
    """
    BATCH_SIZE = get_optimal_batch_size(model_settings)  # åŠ¨æ€ç¡®å®šæ‰¹æ¬¡å¤§å°
    
    # å¦‚æœæµ‹è¯•ç”¨ä¾‹æ•°é‡è¾ƒå°‘ï¼Œç›´æ¥ç”Ÿæˆ
    if num_cases <= BATCH_SIZE:
        return _generate_single_batch_llm(model_settings, num_cases, requirement)
    
    # åˆ†æ‰¹æ¬¡ç”Ÿæˆ
    provider_name = model_settings.get('provider', 'Unknown')
    model_name = model_settings.get('model_name', 'Unknown')
    st.info(f"ğŸ¤– ä½¿ç”¨ {provider_name} - {model_name}")
    st.info(f"ğŸ“¦ å°†åˆ† {(num_cases + BATCH_SIZE - 1) // BATCH_SIZE} ä¸ªæ‰¹æ¬¡ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼ˆæ¯æ‰¹æ¬¡ {BATCH_SIZE} æ¡ï¼‰")
    
    all_test_cases = []
    remaining_cases = num_cases
    batch_num = 1
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    while remaining_cases > 0:
        current_batch_size = min(BATCH_SIZE, remaining_cases)
        
        status_text.text(f"æ­£åœ¨ç”Ÿæˆç¬¬ {batch_num} æ‰¹æ¬¡ ({current_batch_size} ä¸ªæµ‹è¯•ç”¨ä¾‹)...")
        
        try:
            # ç”Ÿæˆå½“å‰æ‰¹æ¬¡
            batch_result = _generate_single_batch_llm(
                model_settings, current_batch_size, requirement, 
                batch_num=batch_num, existing_cases=all_test_cases
            )
            
            if hasattr(batch_result, 'test_cases'):
                batch_cases = batch_result.test_cases
            elif isinstance(batch_result, dict) and 'test_cases' in batch_result:
                batch_cases = batch_result['test_cases']
            else:
                batch_cases = []
            
            all_test_cases.extend(batch_cases)
            remaining_cases -= current_batch_size
            
            # æ›´æ–°è¿›åº¦
            progress = (num_cases - remaining_cases) / num_cases
            progress_bar.progress(progress)
            
            st.success(f"âœ… ç¬¬ {batch_num} æ‰¹æ¬¡å®Œæˆï¼Œç”Ÿæˆäº† {len(batch_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            
        except Exception as e:
            st.error(f"âŒ ç¬¬ {batch_num} æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥: {str(e)}")
            # ç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡
        
        batch_num += 1
    
    status_text.text("ç”Ÿæˆå®Œæˆï¼")
    progress_bar.progress(1.0)
    
    # æ¸…ç†nanå€¼å¹¶è¿”å›ç»“æœ
    cleaned_test_cases = []
    for case in all_test_cases:
        if hasattr(case, 'model_dump'):
            case_dict = case.model_dump()
        elif hasattr(case, 'dict'):
            case_dict = case.dict()
        else:
            case_dict = case
        
        # æ¸…ç†å­—å…¸ä¸­çš„nanå€¼
        cleaned_case_dict = {}
        for key, value in case_dict.items():
            if value is None or str(value).lower() in ['nan', 'none', 'null']:
                cleaned_case_dict[key] = ""
            else:
                cleaned_case_dict[key] = str(value) if value is not None else ""
        
        # é‡æ–°åˆ›å»ºTestCaseå¯¹è±¡
        from core.models import TestCase
        cleaned_test_cases.append(TestCase(**cleaned_case_dict))
    
    return TestCases(test_cases=cleaned_test_cases)

def _generate_single_batch_llm(model_settings, num_cases, requirement, batch_num=1, existing_cases=None):
    """
    ç”Ÿæˆå•ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•ç”¨ä¾‹
    """
    try:
        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
        provider = model_settings.get('provider', 'Unknown')
        model_name = model_settings.get('model_name', 'Unknown')
        
        if batch_num == 1:  # åªåœ¨ç¬¬ä¸€æ‰¹æ¬¡æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            st.info(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ– {provider} - {model_name}")
        
        llm_integrator = AppFactory.create_llm_integrator(
            model_settings['provider'],
            model_settings.get('model_name', 'qwen3:4b'),
            model_settings['api_key'],
            model_settings['base_url']
        )
        llm = llm_integrator.get_llm()

        parser = JsonOutputParser(pydantic_object=TestCases)
        
    except Exception as e:
        st.error(f"âŒ åˆå§‹åŒ–LLMå¤±è´¥: {str(e)}")
        raise

    # å¦‚æœæœ‰å·²å­˜åœ¨çš„æµ‹è¯•ç”¨ä¾‹ï¼Œæ·»åŠ å»é‡æŒ‡ä»¤
    existing_cases_instruction = ""
    if existing_cases and len(existing_cases) > 0:
        existing_cases_summary = []
        for i, case in enumerate(existing_cases[-10:], 1):  # åªæ˜¾ç¤ºæœ€è¿‘10ä¸ªç”¨ä¾‹
            if hasattr(case, 'dict'):
                case_dict = case.dict()
            elif hasattr(case, 'model_dump'):
                case_dict = case.model_dump()
            else:
                case_dict = case
            
            title = case_dict.get('æµ‹è¯•ç”¨ä¾‹æ ‡é¢˜', case_dict.get('title', f'ç”¨ä¾‹{i}'))
            existing_cases_summary.append(f"{i}. {title}")
        
        existing_cases_instruction = f"""
ã€å·²æœ‰æµ‹è¯•ç”¨ä¾‹ã€‘
{chr(10).join(existing_cases_summary)}

ã€é‡è¦ã€‘è¯·ç¡®ä¿æœ¬æ‰¹æ¬¡ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ä¸ä¸Šè¿°å·²æœ‰ç”¨ä¾‹ä¸é‡å¤ï¼Œè¦æœ‰ä¸åŒçš„æµ‹è¯•è§’åº¦å’Œåœºæ™¯ã€‚
"""

    template = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æµ‹è¯•å·¥ç¨‹å¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚æè¿°ç”Ÿæˆ {{num_cases}} ä¸ªç»“æ„åŒ–çš„é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ã€‚

ã€é‡è¦æŒ‡ä»¤ã€‘
1. è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªåä¸º 'test_cases' çš„é”®ï¼Œå…¶å€¼æ˜¯æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
2. æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹éƒ½å¿…é¡»åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
3. è¯·ä½¿ç”¨ä¸­æ–‡ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å†…å®¹
4. æ‰€æœ‰å­—æ®µéƒ½å¿…é¡»æœ‰å€¼ï¼Œä¸èƒ½ä¸ºnullã€undefinedæˆ–ç©ºå€¼ï¼Œå¦‚æœæŸä¸ªå­—æ®µæš‚æ—¶æ²¡æœ‰å†…å®¹ï¼Œè¯·ä½¿ç”¨ç©ºå­—ç¬¦ä¸²""
5. ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œæ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µéƒ½ç”¨åŒå¼•å·åŒ…å›´
{existing_cases_instruction}

éœ€æ±‚æè¿°: {{requirement}}

{{format_instructions}}

è¯·ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼š
"""
    
    prompt = ChatPromptTemplate.from_template(
        template,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )

    chain = prompt | llm | parser
    
    try:
        # æ˜¾ç¤ºæ­£åœ¨è°ƒç”¨APIçš„ä¿¡æ¯
        if batch_num == 1:
            st.info(f"ğŸš€ æ­£åœ¨è°ƒç”¨APIç”Ÿæˆ {num_cases} æ¡æµ‹è¯•ç”¨ä¾‹...")
        
        result = chain.invoke({"num_cases": num_cases, "requirement": requirement})
        
        if batch_num == 1:
            st.success(f"âœ… APIè°ƒç”¨æˆåŠŸ")
            
        return result
        
    except Exception as e:
        error_msg = str(e)
        st.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {error_msg}")
        
        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        if "timeout" in error_msg.lower():
            st.warning("â° å¯èƒ½æ˜¯ç½‘ç»œè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
        elif "api" in error_msg.lower() or "key" in error_msg.lower():
            st.warning("ğŸ”‘ å¯èƒ½æ˜¯APIå¯†é’¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥é…ç½®")
        elif "rate" in error_msg.lower() or "limit" in error_msg.lower():
            st.warning("ğŸš¦ å¯èƒ½è§¦å‘äº†APIé€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•")
        
        raise